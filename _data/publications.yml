#- id: id
#  names: Names of authors
#  title: Name of publication
#  link: URL to publication
#  booktitle: Name of conference, proceedings etc.
#  abstract: Abstract
#  location: Location of venue
#  year: year

- id: vakulenko2020question
  names: "Svitlana Vakulenko, Shayne Longpre, <strong>Zhucheng Tu</strong>, Raviteja Anantha"
  title: Question Rewriting for Conversational Question Answering (<i>Preprint</i>)
  link: https://arxiv.org/abs/2004.14652
  abstract: "Conversational question answering (QA) requires answers conditioned on the previous turns of the
  conversation. We address the conversational QA task by decomposing it into question rewriting and question
  answering subtasks, and conduct a systematic evaluation of this approach on two publicly available datasets.
  Question rewriting is designed to reformulate ambiguous questions, dependent on the conversation context,
  into unambiguous questions that are fully interpretable outside of the conversation context. Thereby,
  standard QA components can consume such explicit questions directly. The main benefit of this approach is
  that the same questions can be used for querying different information sources, e.g., multiple 3rd-party QA
  services simultaneously, as well as provide a human-readable interpretation of the question in context.
  To the best of our knowledge, we are the first to evaluate question rewriting on the conversational question
  answering task and show its improvement over the end-to-end baselines. Moreover, our conversational QA
  architecture based on question rewriting sets the new state of the art on the TREC CAsT 2019 dataset with
  a 28% improvement in MAP and 21% in NDCG@3. Our detailed analysis of the evaluation results provide insights
  into the sensitivity of QA models to question reformulation, and demonstrates the strengths and weaknesses of
  the retrieval and extractive QA architectures, that should be reflected in their integration."
  booktitle: arXiv
  location:
  year: April 2020

- id: pouransari2019least
  names: "Hadi Pouransari, <strong>Zhucheng Tu</strong>, and Oncel Tuzel"
  title: Least Squares Binary Quantization of Neural Networks
  link: https://arxiv.org/abs/2001.02786
  abstract: "Quantizing weights and activations of deep neural networks results in significant improvement
  in inference efficiency at the cost of lower accuracy. A source of the accuracy gap between full precision
  and quantized models is the quantization error. In this work, we focus on the binary quantization,
  in which values are mapped to -1 and 1. We provide a unified framework to analyze different scaling strategies.
  Inspired by the pareto-optimality of 2-bits versus 1-bit quantization, we introduce a novel 2-bits quantization
  with provably least squares error. Our quantization algorithms can be implemented efficiently on the hardware
  using bitwise operations. We present proofs to show that our proposed methods are optimal, and also provide
  empirical error analysis. We conduct experiments on the ImageNet dataset and show a reduced accuracy gap when
  using the proposed least squares quantization algorithms."
  booktitle: The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops
  location: Seattle, USA
  year: June 2020

- id: longpre2019exploration
  names: "Shayne Longpre*, Yi Lu*, <strong>Zhucheng Tu*</strong>, and Chris DuBois"
  title: An Exploration of Data Augmentation and Sampling Techniques for Domain-Agnostic Question Answering
  link: https://arxiv.org/abs/1912.02145
  abstract: "To produce a domain-agnostic question answering model for the Machine Reading Question Answering (MRQA)
  2019 Shared Task, we investigate the relative benefits of large pre-trained language models,
  various data sampling strategies, as well as query and context paraphrases generated by back-translation.
  We find a simple negative sampling technique to be particularly effective, even though it is typically used
  for datasets that include unanswerable questions, such as SQuAD 2.0.
  When applied in conjunction with per-domain sampling, our XLNet (Yang et al., 2019)-based submission achieved
  the second best Exact Match and F1 in the MRQA leaderboard competition."
  booktitle: "Proceedings of the 2nd Workshop on Machine Reading for Question Answering"
  location: Hong Kong, China
  year: November 2019

- id: tu2018pay
  names: "<strong>Zhucheng Tu</strong>, Mengping Li, and Jimmy Lin"
  title: Pay-Per-Request Deployment of Neural Network Models Using Serverless Architectures
  link: http://aclweb.org/anthology/N18-5002
  abstract: "We demonstrate the serverless deployment of neural networks for model
  inferencing in NLP applications using Amazon's Lambda service for
  feedforward evaluation and DynamoDB for storing word embeddings. Our
  architecture realizes a pay-per-request pricing model, requiring zero
  ongoing costs for maintaining server instances. All virtual
  machine management is handled behind the scenes by the cloud provider
  without any direct developer intervention. We describe a number of
  techniques that allow efficient use of serverless resources, and
  evaluations confirm that our design is both scalable and inexpensive."
  booktitle: "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations"
  location: New Orleans, USA
  year: June 2018

- id: liang2018cnns
  names: "Yiyun Liang, <strong>Zhucheng Tu</strong>, Laetitia Huang, and Jimmy Lin"
  title: "CNNs for NLP in the Browser: Client-Side Deployment and Visualization Opportunities"
  link: http://aclweb.org/anthology/N18-5013
  abstract: "We demonstrate a JavaScript implementation of a convolutional neural
  network that performs feedforward inference completely in the browser.
  Such a deployment means that models can run completely on the client,
  on a wide range of devices, without making backend server
  requests. This design is useful for applications with stringent
  latency requirements or low connectivity. Our evaluations show the
  feasibility of JavaScript as a deployment target.
  Furthermore, an in-browser implementation enables
  seamless integration with the JavaScript ecosystem for information
  visualization, providing opportunities to visually inspect neural
  networks and better understand their inner workings."
  booktitle: "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations"
  location: New Orleans, USA
  year: June 2018

- id: tu2018experimental
  names: "<strong>Zhucheng Tu</strong>"
  title: An Experimental Analysis of Multi-Perspective Convolutional Neural Networks
  link: https://uwspace.uwaterloo.ca/handle/10012/13297
  abstract: "Modelling the similarity of sentence pairs is an important problem in
  natural language processing and information retrieval, with applications in tasks
  such as paraphrase identification and answer selection in question answering.
  The Multi-Perspective Convolutional Neural Network (MP-CNN) is a model that improved
  previous state-of-the-art models in 2015 and has remained a popular model for sentence
  similarity tasks. However, until now, there has not been a rigorous study of how the
  model actually achieves competitive accuracy. In this thesis, we report on a series
  of detailed experiments that break down the contribution of each component of MP-CNN
  towards its statistical accuracy and how they affect model robustness. We find that
  two key components of MP-CNN are non-essential to achieve competitive accuracy and
  they make the model less robust to changes in hyperparameters."
  booktitle: "Master's thesis, University of Waterloo"
  location: Waterloo, Canada
  year: May 2018

- id: tang2017experimental
  names: "Raphael Tang, Weijie Wang, <strong>Zhucheng Tu</strong>, and Jimmy Lin"
  title: An Experimental Analysis of the Power Consumption of Convolutional Neural Networks for Keyword Spotting
  link: https://arxiv.org/abs/1711.00333
  abstract: "Nearly all previous work on small-footprint keyword spotting with
  neural networks quantify model footprint in terms of the number of
  parameters and multiply operations for a feedforward inference pass. These values
  are, however, proxy measures since empirical performance in actual
  deployments is determined by many factors. In this paper, we study the
  power consumption of a family of convolutional neural networks for
  keyword spotting on a Raspberry Pi. We find that both proxies are good
  predictors of energy usage, although the number of multiplies is more
  predictive than the number of model parameters. We also confirm that models
  with the highest accuracies are, unsurprisingly, the most power hungry."
  booktitle: "Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2018)"
  location: Calgary, Canada
  year: April 2018

- id: tu2017exploration
  names: "<strong>Zhucheng Tu</strong>, Matt Crane, Royal Sequiera, Junchen Zhang, and Jimmy Lin"
  title: An Exploration of Approaches to Integrating Neural Reranking Models in Multi-Stage Ranking Architectures
  link: https://arxiv.org/abs/1707.08275
  abstract: "We explore different approaches to integrating a simple convolutional
  neural network (CNN) with the Lucene search engine in a multi-stage
  ranking architecture. Our models are trained using the PyTorch deep
  learning toolkit, which is implemented in C/C++ with a Python
  frontend. One obvious integration strategy is to expose the neural network
  directly as a service. For this, we use Apache Thrift, a software
  framework for building scalable cross-language services.
  In exploring alternative architectures,
  we observe that once trained, the feedforward evaluation of neural
  networks is quite straightforward. Therefore, we can extract the parameters of
  a trained CNN from PyTorch and import the model into Java, taking
  advantage of the Java Deeplearning4J library for feedforward evaluation.
  This has the advantage that the entire end-to-end system can be implemented in Java. As a
  third approach, we can extract the neural network from PyTorch and
  \"compile\" it into a C++ program that exposes a Thrift service.
  We evaluate these alternatives in terms of performance (latency and throughput) as well
  as ease of integration.
  Experiments show that feedforward evaluation of the convolutional
  neural network is significantly slower in Java, while the performance of
  the compiled C++ network does not consistently beat the PyTorch implementation."
  booktitle: "Proceedings of the SIGIR 2017 Workshop on Neural Information Retrieval (Neu-IR'17)"
  location: Tokyo, Japan
  year: August 2017

- id: sequiera2017exploring
  names: "Royal Sequiera, Gaurav Baruah, <strong>Zhucheng Tu</strong>, Salman Mohammed, Jinfeng Rao, Haotian Zhang, and Jimmy Lin"
  title: Exploring the Effectiveness of Convolutional Neural Networks for Answer Selection in End-to-End Question Answering
  link: https://arxiv.org/abs/1707.07804
  abstract: "Most work on natural language question answering today focuses on
  answer selection: given a candidate list of sentences, determine
  which contains the answer. Although important, answer selection is
  only one stage in a standard end-to-end question answering
  pipeline. This paper explores the effectiveness of convolutional neural
  networks (CNNs) for answer selection in an end-to-end context using
  the standard TrecQA dataset.
  We observe that a simple <i>idf</i>-weighted word overlap
  algorithm forms a very strong baseline, and that despite substantial
  efforts by the community in applying deep learning to tackle answer selection,
  the gains are modest at best on this dataset.
  Furthermore, it is unclear if a CNN is more effective than the baseline
  in an end-to-end context based on
  standard retrieval metrics. To further explore this finding, we conducted
  a manual user evaluation, which confirms that
  answers from the CNN are detectably better than those from <i>idf</i>-weighted
  word overlap. This result suggests that users are sensitive to relatively small
  differences in answer selection quality."
  booktitle: "Proceedings of the SIGIR 2017 Workshop on Neural Information Retrieval (Neu-IR'17)"
  location: Tokyo, Japan
  year: August 2017

- id: lin2016prizm
  names: "Jimmy Lin, <strong>Zhucheng Tu</strong>, Michael Rose, and Patrick White"
  title: "Prizm: A Wireless Access Point for Proxy-Based Web Lifelogging"
  link: https://dl.acm.org/authorize?N25521
  abstract: "We present Prizm, a prototype lifelogging device that comprehensively
  records a user’s web activity. Prizm is a wireless
  access point deployed on a Raspberry Pi that is designed
  to be a substitute for the user’s normal wireless access point.
  Prizm proxies all HTTP(S) requests from devices connected
  to it and records all activity it observes. Although this particular
  design is not entirely novel, there are a few features
  that are unique to our approach, most notably the physical
  deployment as a wireless access point. Such a package allows
  capture of activity from multiple devices, integration
  with web archiving for preservation, and support for offline
  operation. This paper describes the design of Prizm, the
  current status of our project, and future plans."
  booktitle: Proceedings of the First Workshop on Lifelogging Tools and Applications (LTA 2016)
  location: Amsterdam, The Netherlands
  year: October 2016
